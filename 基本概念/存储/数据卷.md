# 数据卷

容器中的磁盘文件都是短暂的，这样就为一些特殊的应用运行在容器中带来了一些困扰。首先呢，当容器挂了，kubelet就要重启它，但是文件就都没了——容器以全新的状态启动。第二，当Pod中运行多个容器的时候，常常需要共享一些文件。`Volume`的出现解决了这两个问题。

建议你先去学习完[Pod](../业务组件/泡德（Pod）/Pod.md)再过来。

## 背景

Docker也有[数据卷](https://docs.docker.com/storage/)的概念，尽管相对来说更加简单，管理也比较少。在Docker里，一个数据卷基本上就是一个磁盘目录或者是在另一个容器里。其生命周期不受管理，而且直到最近依然只支持基于本地磁盘的数据卷。Docker现在提供了数据卷驱动，但是功能很有限（比如到Docker 1.7每个容器只能用一个数据卷驱动，而且还无法给数据卷传递参数）。

但在k8s中的数据卷就不一样了，哈呀，这就要开始吹牛逼了，它是有明确生命周期的——和围绕它的Pod保持一致。这就是说，这里的数据卷脱离了容器，而是关联到了Pod这一层面上，因此在容器重启的过程中数据依然能够保留下来。当然，如果Pod退出了，那数据卷也就跟着退出了。更重要的是，k8s还能支持好多好多种的数据卷，一个Pod可以同时使用任意数量的各种类型的数据卷。

数据卷的核心就是一个目录，可能还包含一些数据在里面，然后Pod中的容器可以访问到这些数据和目录。这个目录怎么来、中间介质是怎样的，以及其中包含的东西，都是由特定的数据卷类型来决定的。

要使用数据卷，Pod需要定义用到的数据卷（`.spec.volumes`字段），以及这些数据卷要挂载到容器的什么地方去（`.spec.containers[*].volumeMounts`字段）。

容器中的一个进程看到的文件系统是由Docker镜像和数据卷组合而成的。[Docker镜像](https://docs.docker.com/get-started/)位于文件系统的最底层，任何数据卷都是挂载到镜像的特定位置上的。Pod中的每个容器必须要单独声明数据卷的挂载位置。

## 数据卷类型

k8s支持好多种数据卷：

- [awsElasticBlockStore]()
- [azureDisk]()
- [azureFile]()
- [cephfs]()
- [cinder]()
- [configMap](#configmap)
- [csi]()
- [downwardAPI](#downwardapi)
- [emptyDir](#emptydir)
- [fc (fibre channel)]()
- [flexVolume]()
- [flocker]()
- [gcePersistentDisk]()
- [gitRepo (deprecated)]()
- [glusterfs]()
- [hostPath](#hostpath)
- [iscsi]()
- [local](#local)
- [nfs](#nfs)
- [persistentVolumeClaim](#persistentvolumeclaim)
- [projected](#projected)
- [portworxVolume]()
- [quobyte]()
- [rbd]()
- [scaleIO]()
- [secret](#secret)
- [storageos]()
- [vsphereVolume]()

我们欢迎大家贡献其他的类型。在这里，由于数据卷类型太多了，我只翻译一下常用的，或者说是我个人感兴趣的部分。

### configMap

[`configMap`](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/)提供了一种将配置信息注入到Pod中的方法。保存在`ConfigMap`对象中的数据可以通过`configMap`类型的数据卷来引用，被Pod中的容器化应用所使用。

当你需要引用一个`configMap`对象，在数据卷中指定名字即可。还可以自定义要使用的ConfigMap中的某个路径。比如要将名为`log-config`的ConfigMap挂载到一个叫`configmap-pod`的Pod中，YAML可以这样写：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level
```

`log-config`被当成了一个数据卷，它的`log_level`记录中保存的内容被挂载到了容器中的“`/etc/config/log_level`”路径上。注意这个路径是基于数据卷的`mountPath`和`path`的名字`log_level`组成的。

>**小心**：使用[ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/)必须先创建它。

>**注意**：如果容器将ConfigMap作为[subPath](#使用subPath)数据卷进行挂载，是无法感知到ConfigMap的更新的。

>**注意**：文本数据是作为UTF-8编码的文件暴露出来的。要使用其他编码的话，请使用binaryData。

### downwardAPI

`downwardAPI`数据卷用来将downward API数据提供给应用。它挂载到一个目录然后将需要的数据写入到普通本文文件汇总。

>**注意**：容器如果将Downward API作为[subPath](#使用subPath)数据卷挂载，将无法感知到Downward API的更新。

### emptyDir

`emptyDir`数据卷会在Pod指派到一个节点后进行首次创建，只要Pod还在那个节点，那这个数据卷就一直在。人如其名，它一开始是空的。Pod中的容器们可以读写`emptyDir`书卷中的同样的文件，尽管同一个数据卷可能挂载到不同容器的不同路径上。当Pod从一个节点移除了，`emptyDir`中的数据就永久删除了。

>**注意**：容器的崩溃*不会*导致Pod从一个节点移除，所以容器崩溃并不会丢失`emptyDir`中的数据。

一些使用`emptyDir`的场景：

- 当缓存，比如基于磁盘的合并排序
- 某个漫长计算中的检查点，为了在崩溃后进行恢复
- 一个数据容器往里灌数据，另一个web服务容器向外提供这些数据

默认情况下`emptyDir`数据卷存储在节点使用的存储介质上，可能是磁盘、SSD活网络存储，视你的情况而定。但你也可以将`emptyDir.medium`设置为`“Memory”`，让k8s给你挂载一个tmpfs（基于RAM的文件系统）。tmpfs肯定非常快了，但是要知道跟磁盘不同的是，节点重启的话tmpfs会被清掉，而且里面的数据受你的容器的内存大小限制。

#### 栗子

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
```

### hostPath

一个`hostPath`数据卷就是把宿主机文件系统的一个文件或目录挂载到Pod里。这可能并不是大部分Pod所需要的，但对于个别应用来说，可就真是雪中送炭呢，还真是厉害呢。

比如一些`hostPath`的场景：

- 需要运行一个访问Docker的容器；挂载一个`/var/lib/docker`的`hostPath`
- 在容器中运行cAdvisor；挂载`sys`的`hostPath`
- 允许Pod定义一个`hostPath`是否应该在Pod运行前就存在，是否要创建，以及它存在的形式。这。。。。没太理解。

除了必填的`path`属性，用户还可以给`hostPath`数据卷指定一个`type`。

`type`支持的值有：

**值**|**含义**
-|-
&nbsp;|空字符串（默认）用来向后兼容，在挂载hostPath数据卷之前不会做任何检查。
`DirectoryOrCreate`|如果指定路径不存在，会创建一个空目录，权限设置为0755，和kubelet保持同样的用户组和用户归属。
`Directory`|指定路径必须存在一个目录
`FileOrCreate`|如果指定路径不存在，会创建一个空文件，权限设置为0644，和kubelet保持同样的用户组和用户归属。
`File`|指定路径必须存在一个文件
`Socket`|指定路径必须存在一个UNIX socket。
`CharDevice`|指定路径必须存在一个字符设备
`BlockDevice`|指定路径必须存在一个块设备

使用这些类型的时候要注意，因为：

- 相同配置的Pod（比如基于podTemplate创建）可能会在不同节点上由于不同的文件产生不同的行为
- 当k8s进行资源相关的调度时，它是无法顾及到`hostPath`所使用的资源的
- 在宿主机上创建的文件或目录只对root可写。要么你就运行一个[特权容器](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)的root进程，要么就修改宿主机的对应文件权限，让它在`hostPath`数据卷中提供可写权限

#### 栗子

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
```

>**小心**：注意`FileOrCreate`模式并不会创建上级目录。如果上级目录不存在，Pod就报错起不来。要想确保这种模式正常工作，可以分别挂载目录和文件，如下面的栗子。

#### FileOrCreate栗子

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-webserver
spec:
  containers:
  - name: test-webserver
    image: k8s.gcr.io/test-webserver:latest
    volumeMounts:
    - mountPath: /var/local/aaa
      name: mydir
    - mountPath: /var/local/aaa/1.txt
      name: myfile
  volumes:
  - name: mydir
    hostPath:
      # Ensure the file directory is created.
      path: /var/local/aaa
      type: DirectoryOrCreate
  - name: myfile
    hostPath:
      path: /var/local/aaa/1.txt
      type: FileOrCreate
```

### local

**功能状态**：`Kubernetes v1.14 [stable]`

一个`local`数据卷就是挂载一个本地存储设备，比如一个磁盘、分区或目录。

本地（local）数据卷只能用作一个静态创建的持久卷（PersistentVolume）。目前不支持动态创建。

和`hostPath`相比，本地数据卷可以作为一个持久的、可移植的行为，不需要手动将Pod调度到节点，因为系统会根据持久卷的节点亲和性，知道数据卷的节点约束。

但是，本地数据卷依然要依赖底层节点的可用性，并不适用于所有应用。如果节点闹妖了，本地数据卷也就无法访问了，使用它的Pod也就无法运行了。使用本地数据卷的应用必须要能接受这种低可用性，以及可能发生的数据丢失，这些都要依赖于底层磁盘的持久化特性。

下面是一个持久卷定义，使用了`local`数据卷，包含了`nodeAffinity`属性：

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
```

使用本地数据卷的时候，持久卷必须要有`nodeAffinity`属性。它可以让k8s调度根据本地数据卷的约束，把Pod调度到正确的节点上。

持久卷的`volumeMode`可以设置为“Block”（默认是“Filesystem”），这样可以将本地数据卷暴露成一个裸设备。

当你使用本地数据卷的时候，建议创建一个StorageClass，并将`volumeBindingMode`设置为`WaitForFirstConsumer`。见[这个栗子](Storage%20Class.md#local)。延迟数据卷绑定可以确保持久卷Claim（PersistentVolumeClaim，PVC）在绑定时参考Pod相关的节点约束，比如节点资源要求、节点选择器、Pod亲和性以及Pod反亲和性。

为了更好的管理本地数据卷的生命周期，可以单独运行一个外部的静态分配器（provisioner）。注意这种分配器目前依然是不能支持动态分配的。关于如何运行外部的本地分配器，见[本地数据卷分配器指南](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner)。

>**注意**：如果没有使用外部的静态分配器来管理数据卷的生命周期，那么本地持久卷就需要用户进行手动的清理和删除。

### nfs

`nfs`数据卷可以使用已有的NFS（网络文件系统）为Pod提供共享挂载。和`emptyDir`不同，Pod没了就被删，nfs的数据是一直保留的，仅仅是数据卷取消了挂载。这就意味着NFS数据卷可以预先往里保存一些数据，这些数据就可以在Pod间进行“传递”了。NFS可以同时挂载多个写入端。

>**小心**：使用NFS前必须让NFS服务器开启目录共享。

更多小知识见[NFS栗子](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs)。

### persistentVolumeClaim

一个`persistentVolumeClaim`用来把[`持久卷（PersistentVolume）`](持久卷（Persistent%20Volume）.md)挂载到Pod中。持久卷提供了一个让用户可以“索取（claim）”持久化存储的方式（比如GCE的PersistentDisk或iSCSI数据卷），而不需要知道具体云环境的细节。

更多小知识见[持久卷示例]()。

### projected

`projected`数据卷可以将多个已存在的数据卷映射到同一个目录中。

当前支持映射以下类型的数据卷：

- [secret](#secret)
- [downwardAPI](#downwardapi)
- [configMap](#configmap)
- `serviceAccountToken`

这些源数据卷必须得和Pod在相同的命名空间中。更多细节请看[大一统数据卷设计](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md)。

service account token的映射是从1.11版本引入的功能，在1.12版本中进入到Beta阶段。要想在1.11版本中启用这个功能，需要将`TokenRequestProjection`[特性门](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/)设置为True。

#### secret、downwardAPI以及configmap的大一统栗子

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - downwardAPI:
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "cpu_limit"
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
```

#### 多个secret以及权限设置。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - secret:
          name: mysecret2
          items:
            - key: password
              path: my-group/my-password
              mode: 511
```

每个被映射的源数据卷都要列在`sources`下面。参数基本相同，除了：

- 对于secret，`secretname`字段要改成`name`，和ConfigMap的命名方式保持一致。
- `defaultMode`只能设置在projected那一层，不能给每个源数据卷单独设置。但是如上面的栗子，可以给每个映射单独设置`mode`。

当你开启了`TokenRequestProjection`特性，可以为当前[service account](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens)将token注入到Pod的指定路径上。如下面的栗子：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sa-token-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: token-vol
      mountPath: "/service-account"
      readOnly: true
  volumes:
  - name: token-vol
    projected:
      sources:
      - serviceAccountToken:
          audience: api
          expirationSeconds: 3600
          path: token
```

栗子中的Pod包含了一个映射的数据卷，注入了service account token。Pod中的容器可以用这个token来访问k8s的api server。`audience`字段定义了token的受众。token的接收者必须用token的受众标识来表明自己的身份，否则就应该拒绝这个token。该字段可选，默认是api server的身份标识。

`expirationSeconds`是这个service account token的有效时长。默认是1小时，最少是10分钟（600秒）。管理员可以通过apiserver的`--service-account-max-token-expiration`选项来修改这个最大值限制。`path`字段定义了相对于挂载点的路径。

>**注意**：如果容器将projected数据卷作为[subPath](#使用subPath)容器，将无法感知到源数据卷发生的变化。

### secret

人如其名，`secret`数据卷是用来传递敏感信息到Pod中，比如密码。可以用k8s的API来保存secret，然后将它们作为文件挂载到Pod中，无需跟k8s直接耦合。`secret`数据卷底层是tmpfs（基于RAM的文件系统），所以它们永远不会写入非易失存储中。

>**小心**：secret使用前要先用k8s API进行创建。

>**注意**：如果容器将Secret作为[subPath](#使用subPath)数据卷挂载，将无法感知到Secret的更新。

关于Secret的详细介绍在[这里](../配置/Secret.md)。

## 使用subPath

有的时候，可能需要在一个Pod中共享一个数据卷来实现多种用途。`volumeMounts.subPath`属性可以对引用的数据卷指定一个子路径（sub-path），而不是直接用它的根路径。

下面是一个基于LAMP（Linux Apache Mysql PHP）的Pod，它就使用了一个可以共享的数据卷。HTML文件映射到了`html`目录，数据库则是在`mysql`目录：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-lamp-site
spec:
    containers:
    - name: mysql
      image: mysql
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: "rootpasswd"
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql
    - name: php
      image: php:7.0-apache
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-lamp-site-data
```

### 加入环境变量

**功能状态**：`Kubernetes v1.17 [stable]`

使用`subPathExpr`可以通过反向API的环境变量来构建`subPath`的目录。使用这个功能需要打开`VolumeSubpathEnvExpansion`[特性门](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/)。从1.15版本开始它是默认开启的。`subPath`和`subPathExpr`属性是互斥的。

在下面的例子中，Pod使用了一个hostPath数据卷，用`subPathExpr`创建了一个`pod1`目录，通过反向API得到了Pod的名字。在容器内部，宿主机的`/var/log/pods/pod1`目录挂载到了容器的`/logs`目录上。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: container1
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    image: busybox
    command: [ "sh", "-c", "while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt" ]
    volumeMounts:
    - name: workdir1
      mountPath: /logs
      subPathExpr: $(POD_NAME)
  restartPolicy: Never
  volumes:
  - name: workdir1
    hostPath:
      path: /var/log/pods
```

## 资源

`emptyDir`数据卷的存储媒介（磁盘、SSD等）是由kubelet所在根目录（一般是`/var/lib/kubelet`）的文件系统使用的媒介来决定的。对于`emptyDir`和`hostPath`数据卷来说，不会对它们可以使用的空间进行限制，容器之间或者Pod之间也没有什么隔离。

在未来的日子里，我们希望`emptyDir`和`hostPath`能够通过[resource](../配置/管理容器资源.md)定义来主动申请一个固定大小的空间，而且如果集群有多种媒介，还能选择想使用的媒介类型，而且还能发射激光炮，还能变身超级赛亚人。

## 非主流数据卷插件

非主流（Out-of-tree）数据卷插件包括了容器存储接口（Container Storage Interface，CSI）以及FlexVolume。它们可以让存储提供商开发自己的自定义存储插件，而不用把它们加到k8s的仓库中。

在CSI和FlexVolume出现之前，所有的数据卷插件（比如上面列出的各种数据卷类型）都是“主流的”，意味着它们都是跟k8s核心二进制一起构建、链接、编译和发布的，扩展了k8s的核心API。这就意味着如果要在k8s中添加一个新的存储系统（数据卷插件），需要将代码发布到k8s的核心代码仓库中。

CSI和FlexVolume都可以让数据卷插件的开发独立于k8s的核心代码，作为扩展部分部署（安装）到k8s集群中。

如果你是存储供应商，想开发非主流的数据卷插件，请移步[FAQ](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md)。

### CSI

[容器存储接口](https://github.com/container-storage-interface/spec/blob/master/spec.md)（CSI）为容器编排系统（比如k8s）提供了标准接口，可以将各种存储系统暴露到容器中。

可以去学习以下[CSI设计提案](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md)。

CSI是在k8s的1.9版本中作为alpha版本引入的，在1.10中升级为beta版本，在1.13中升级为GA。

>**注意**：从k8s的1.13版本开始，CSI规范的0.2和0.3版本被认为是过期版本，未来可能会删除对这些版本的支持。

>**注意**：CSI驱动可能无法兼容所有k8s版本。请仔细阅读每个k8s版本的CSI驱动文档，了解其部署方式，以及兼容性说明。

当k8s集群中部署了一个兼容CSI的数据卷驱动后，用户就可以使用`csi`这种数据卷类型来进行添加、挂载等，这些数据卷都是通过CSI驱动来暴露的。

`csi`数据卷不支持在Pod中直接引用，应该是只能通过`PersistentVolumeClaim`对象来进行引用。

存储管理员可以使用以下字段来配置一个CSI持久卷：

- `driver`：一个字符串值，用来声明要使用的数据卷驱动的名字。这个值必须要跟CSI驱动中定义的`GetPluginInfoResponse`返回值能对应的上，见[CSI定义](https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo)。这个字段是让k8s能够知道应该调用哪个CSI驱动，同时也能让CSI驱动组件知道哪个PV对象归属于这个CSI驱动。
- `volumeHandle`：一个字符串值，唯一标识出这个数据卷。这个值必须要跟CSI驱动中定义的`CreateVolumeResponse`中的`volume.id`能够对应的上，见[CSI定义](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume)。当引用数据卷时，这个值要放在`volume_id`中去请求CSI数据卷驱动。
- `readOnly`：一个可选的布尔值，表示这个数据卷是否要以只读形式来进行“ControllerPublished”（attached）。默认是false。在`ControllerPublishVolumeRequest`中这个值要放在`readonly`字段中发送给CSI驱动。
- `fsType`：如果PV的`VolumeMode`是`Filesystem`，那么这个字段就可以用来声明要用来挂载该数据卷的文件系统。如果数据卷还未格式化且支持格式化，就会根据这个字段对数据卷进行格式化。这个字段要放在`ControllerPublishVolumeRequest`、`NodeStageVolumeRequest`以及`NodePublishVolumeRequest`的`VolumeCapability`字段中发送给CSI驱动。
- `volumeAttributes`：一个字符串到字符串的映射，定义数据卷的静态属性。这一套映射必须要跟CSI驱动中定义的`CreateVolumeResponse`返回值里面的`volume.attributes`能对应上，见[CSI定义](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume)。这个映射要放在`ControllerPublishVolumeRequest`、`NodeStageVolumeRequest`以及`NodePublishVolumeRequest`中的`volume_context`字段中发送给CSi驱动。
- `controllerPublishSecretRef`：引用一个包含敏感信息的secret对象，发送给CSI驱动，从而可以完成CSI`ControllerPublishVolume`和`ControllerUnpublishVolume`调用。这是个可选字段，如果不需要secret，也可以为空。如果secret对象中包含了多个secret，所有的secret都会传过去。
- `nodeStageSecretRef`：引用一个包含敏感信息的secret对象，发送给CSI驱动，从而可以完成CSI`NodeStageVolume`调用。这是个可选字段，如果不需要secret，也可以为空。如果secret对象中包含了多个secret，所有的secret都会传过去。
- `nodePublishSecretRef`：引用一个包含敏感信息的secret对象，发送给CSI驱动，从而可以完成CSI`NodePublishVolume`调用。这是个可选字段，如果不需要secret，也可以为空。如果secret对象中包含了多个secret，所有的secret都会传过去。

#### CSI裸设备数据卷

**功能状态**：`Kubernetes v1.18 [stable]`

外部CSI驱动的提供者可以为k8s中的应用实现裸设备数据卷支持。

你也可以像往常一样[创建基于裸设备的PV/PVC](持久卷（Persistent%20Volume）.md#裸设备数据卷)，不需要任何CSI特定的修改。

#### CSI临时数据卷

**功能状态**：`Kubernetes v1.16 [beta]`

这个功能可以让CSI数据卷直接写在Pod定义中，而不是通过一个PersistentVolume（持久卷）。这种方式定义的数据卷都是临时数据卷，如果Pod重启，数据则消失。

栗子：

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: my-csi-app
spec:
  containers:
    - name: my-frontend
      image: busybox
      volumeMounts:
      - mountPath: "/data"
        name: my-csi-inline-vol
      command: [ "sleep", "1000000" ]
  volumes:
    - name: my-csi-inline-vol
      csi:
        driver: inline.storage.kubernetes.io
        volumeAttributes:
          foo: bar
```

这个功能需要打开`CSIInlineVolume`特性门。从k8s 1.16版本开始默认开启。

只有一部分CSI驱动能够支持CSI临时数据卷。完整清单在[这里](https://kubernetes-csi.github.io/docs/drivers.html)。

# 开发者资源

关于如何开发一个CSI驱动的详细信息，请参考[k8s csi文档](https://kubernetes-csi.github.io/docs/)

#### 从主流插件迁移到CSI驱动

**功能状态**：`Kuberneetes v1.14 [alpha]`

当开启了CSI迁移功能后，会将对主流插件的操作定向到对应的CSI驱动上（当然得提前安装和配置好了）。其中实现了必须的转换逻辑和其他功能，实现对操作的无缝衔接和重新路由。所以在转换到一个CSI驱动的时候，操作者不需要对已有的Storage Class、PV或PVC（引用了主流插件）进行任何配置变更。

在alpha版本中，支持的操作和特定包括提供（provisioning）/删除、attach/detach、挂载/卸载以及数据卷大小调整。

可以支持CSI迁移并且有对应的CSI驱动实现的主流插件已经列在了上面的“数据卷类型”一节中。

### FlexVolume

FlexVolume是一个非主流插件接口，从k8s 1.2版本开始就有了（比CSI早）。它是基于exec模式来跟驱动进行交互的。FlexVolume驱动的二进制必须安装在每个节点的预定义数据卷插件路径中（某些情况下是master节点）。

Pod跟FlexVolume驱动交互的时候是通过`flexvolume`这个主流插件。详见[这里](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md)。

## 挂载传播

挂载传播可以让一个容器挂载的数据卷共享给Pod中的其他容器，甚至同一个节点上的其它Pod。

数据卷的挂载传播是由容器的volumeMounts中的`mountPropagation`属性来控制的。它的值包括：

- `None`——该数据卷的挂载无法感知后续任何挂载，或者是同主机下其子目录的挂载。容器创建的挂载在宿主机上是不可见的。默认就是这样。

这种模式跟[Linux内核文档](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)中描述的`private`挂载传播是一样的。

- `HostToContainer`——该数据卷挂载可以感知到后续挂载以及它的子目录。

换句话说，如果苏初级在该数据卷挂载内部又挂载了别的东西，容器是可以看到的。

类似的，如果Pod基于`Bidirectional`挂载传播挂载到了同一个数据卷，`HostToContainer`挂在传播的容器是可以看到它的。这里翻译的有点懵逼，需要实践一下，原文没有给栗子。

这种模式跟[Linux内核文档](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)中描述的`rslave`挂载传播是一样的。

- `Bidirectional`——这种数据卷挂载行为和`HostToContainer`是一样的。除此之外，容器创建的所有数据卷挂载都会反向传播给宿主机，以及使用同一个数据卷的其他所有Pod和容器。

这种模式的一个常见场景就是一个使用了FlexVolume或CSI驱动的Pod，或者是需要使用`hostPath`数据卷来挂载宿主机上的一些东西的Pod。

这种模式跟[Linux内核文档](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)中描述的`rshared`挂载传播是一样的。

>**小心**：`Bidirectional`挂载传播是有风险的。它会损害到宿主机的操作系统，因此只能用于特权容器。强烈建议你先去熟悉Linux的内核行为。此外，在容器完犊子（termination）的时候，必须要销毁（unmount）所有由容器创建的数据卷挂载。

### 配置

在某些部署场景中（CoreOS、RedHat/Centos、Ubuntu）要想让挂载传播正常工作，必须要像下面这样正确配置Docker的挂载共享。

修改Docker的`systemd`服务文件。设置`MountFlags`：

```properties
MountFlags=shared
```

如果有`MountFlags=slave`还要删掉。然后重启Docker：

```shell script
sudo systemctl daemon-reload
sudo systemctl restart docker
```

## 下一步……

- 学习实例[使用持久卷（PersistentVolume）来部署WordPress和MySQL](https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/)。