# 节点相关的数据卷限制

本文介绍对于不同云服务商，节点可以添加的数据卷数量上限。

像Google、Amazon以及Microsoft，一般都会限制一个节点可添加的最大的数据卷数量上限。k8s肯定是要尊重这些限制的。否则Pod调度到一个节点之后可能会因为添加数据卷的问题被卡住。

## k8s的默认限制

对于每个节点可以添加的数据卷最大数量，k8s是有默认上限的：

云服务商|单节点数据卷上限
-|-
[Amazon EBS]()|39
[Google Persistent Disk]()|16
[Microsoft Azure Disk Storage]()|16

这些我都不感兴趣，所以都没写。

## 自定义上限

可以通过`KUBE_MAX_PD_VOLS`环境变量来修改这些限制，改完之后再启动调度器。CSI驱动的话过程不太一样，可以去看他们的文档了解如果修改限制。

如果你设置的值高于默认值，那你多加小心。去看看云服务商的文档，确认他们能支持你设置的上限值。

这个限制作用于整个集群，会影响所有节点。

## 动态数据卷限制

**功能状态**：`Kubernetes v1.17 [stable]`

以下数据卷类型可以支持动态数据卷限制：

- Amazon EBS
- Google Persistent Disk
- Azure Disk
- CSI

对于主流数据卷插件，k8s自动判断节点类型并为节点设置合适的数据卷上限。比如：

- 在[GCE]()上面，[基于节点类型]()，一个节点最多可以有127个数据卷。
- 对于在M5、C5、R5、T3以及Z1D实例类型上的Amazon EBS，k8s允许一个节点最多添加25个数据卷。对于[EC2]()上的其他实例类型，k8s允许一个节点最多添加39个数据卷。
- 在Azure上，基于节点类型，一个节点最多可以有64个磁盘。详见[Azure的虚拟机大小]()。
- 如果一个CSI存储驱动发布了其在一个节点上的最大数据卷数量（通过`NodeGetInfo`），[kube-scheduler](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)就会尊重这个限制。详见[CSI规范](https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo)。
- 对于主流插件管理的数据卷，迁移到CSI驱动后，数据卷数量上限是由CSI驱动决定的。