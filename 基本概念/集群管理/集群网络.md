# 集群网络

网络是k8s的核心概念之一，但是研究其原理也是一个很有挑战性的任务。这里要强调4个不同的网络问题：

- 1.高耦合容器对容器通信：这是通过[Pod](../业务组件/泡德（Pod）/Pod.md)和`localhost`通信来解决的。
- 2.Pod to Pod通信：这是本文主要讲的东西。
- 3.Pod to Service通信：这部分在[Service](../Service，负载均衡，网络/Service.md)中。
- 4.外部到Service的通信：这部分也在[Service](../Service，负载均衡，网络/Service.md)中。

k8s的关键就是在多个应用之间共享机器。一般来说共享机器的话两个应用就不能用同样的端口。在多个开发者之间协调端口非常困难，而且还将用户暴露在他们无法控制的集群级别的问题上。

而如果要用动态端口分配，也会给系统带来一堆并发症——每个应用要添加端口选项，apiserver要知道如何将动态端口号插入到配置中，Service要知道如何找到彼此，等等。实在是不想处理这些烂事儿，k8s选了一条不同的路。

## k8s网络模型

每个`Pod`有自己的IP。这就意味着你不需要在`Pod`之间创建显式的链接，而且几乎用不着去做容器和主机的端口映射。这样，从端口分配、命名、服务发现、负载均衡、应用配置和迁移的角度来看，我们创建了一个干净的、向后兼容的模型，在这种模型下，`Pod`就跟虚拟机或物理机很像了。

对于每一种网络实现方案，k8s都做了以下基本要求（除了一些专门搞的网络分割策略）：

- Pod跟自己同一节点上的其他Pod通信时可以不用NAT
- 节点代理（比如系统守护进程、kubelet）可以跟该节点的所有Pod通信

注意：对于可以支持`Pod`在主机网络中运行的平台（比如Linux）：

- 在主机网络中的Pod跟其他节点的Pod通信时可以不用NAT

这种模型不光是整体上降低了复杂度，从原则上也兼容了k8s希望让应用从虚拟机平滑迁移到容器的想法。比如你的应用之前运行在虚拟机上，虚拟机是有IP的，而且可以跟其他虚拟机通信。这种基本的模型是一致的。

k8s的IP地址是存在于`Pod`中的——`Pod`中的容器共享网络空间——包括IP地址。这就是说一个`Pod`中的容器可以通过`localhost`上的端口进行相互通信。同时也意味着一个`Pod`内部的容器必须要做好端口协调，这个一个虚拟机内的多个进程的关系也是一致的。我们把它叫做“IP-per-pod”模型。

这东西的具体实现要看每个容器运行时了。

可以让`Node`上的端口转发到`Pod`上（称为主机端口），但这是一种很嘚瑟的操作。具体的转发实现同样要依赖于具体的容器运行时。`Pod`本身是不知道有没有主机端口的。

## 如何实现k8s的网络模型

有很多种方式来实现。本文并不是要把每种方法都讲透，但是希望能引导你了解各种不同的技术，作为你人生的一个重要的跳板。

下面的网络选择是按照字母排序的——顺序并不代表什么优先级。

下面的内容本人没有全都翻译，只挑了几个自己喜欢的。

### ACI

[Cisco Application Centric Infrastructure](https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/index.html)提供了一个集成的overlay、underlay SDN解决方案，支持容器、虚拟机和裸金属服务器。[ACI](https://github.com/noironetworks/aci-containers)提供了容器网络到ACI的集成。详见[这里](https://www.cisco.com/c/dam/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/solution-overview-c22-739493.pdf)。

### Antrea

[Antrea](https://github.com/vmware-tanzu/antrea)项目是一个开源的k8s网络方案，目的是做成k8s原生的方案。它用Open vSwitch作为数据面（data plane）。Open vSwitch是一个高性能、可编程的虚拟交换机，支持Liunx和Windows。Open vSwitch使得Antrea实现的k8s网络策略具有高性能、高效率的特点。感谢Open vSwitch“可编程”的特点，让Antrea能够扩展网络和安全特性，在Open vSwtich之上提供服务。

### Flannel

[Flannel](https://github.com/coreos/flannel#flannel)是一个非常简单的overlay网络，满足k8s的需求。许多人都反馈说他们在k8s上面用了Flannel之后走向了人生巅峰。

### L2网络和Linux桥接

如果你有一个“哑（dumb）”L2网络，比如在“裸金属”环境中有一个简单的交换机，那你跟上面GCE的安装有点类似。注意这里给出的方法只是偶尔测过——它看上去能用，但是没有全面测试过。如果你用了这种方式，而且还完善了整个流程，请让我们也见识一下。

可以参见Lars Kellogg-Stedman的[这篇非常nice的教程](https://blog.oddbit.com/post/2014-08-11-four-ways-to-connect-a-docker/)中的“With Linux Bridge devices”一节。

### OpenVSwitch

[OpenVSwitch](https://www.openvswitch.org/)可以说是更加成熟，但是用它构建overlay网络也更加复杂。好多网络“大厂”都用它。

### Project Calico

[Project Calico](https://docs.projectcalico.org/)是一个开源的容器网络服务以及网络策略引擎。

Calico能够把k8s的Pod基于和互联网一样的IP网络策略连接起来，提供了高伸缩性的网络和网络策略解决方案，支持Linux（开源）和Windows（专有——可以从[Tigera](https://www.tigera.io/tigera-products/)获得）。Calico的部署可以不用封装或overlay就能实现高性能、高伸缩性的数据中心网络。通过Calico的分布式防火墙，为k8s的Pod提供了细粒度的基于意图的网络安全策略。

Calico可以跟其他网络方案一起运行在策略强制（enforcement）模式中，比如Flannel、[canal](https://github.com/projectcalico/canal)或原生的GCE、AWS以及Azure网络。

## 下一步……

关于网络模型的早期设计及其基本原理，还有一些未来的计划，都在[网络设计文档](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/network/networking.md)中了。