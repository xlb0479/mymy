# 热脸和冷屁股

[*节点亲和性*](将Pod指派到节点.md#亲和性与反亲和性)是[Pod](../业务组件/泡德（Pod）/Pod.md)的一个属性，可以将它们*吸引（attracts）*到一组[节点](../集群架构/节点（Node）.md)上（可以是优先考虑也可以是强制要求）。*冷屁股（Taints）* 正相反——它可以让节点排斥某些Pod。

*热脸（Tolerations）* 是定义在Pod上的，允许（但不是要求）Pod可以调度到有匹配的冷屁股的节点上。

热脸和冷屁股协同工作，保证Pod不会被调度到不合适的节点上。一个节点可以设置多个冷屁股；如果Pod没有能贴到这个冷屁股的热脸，那就不会被调度到这个节点上。

## 概念

用[kubectl taint](https://v1-18.docs.kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint)为节点设置冷屁股。比如：

```shell script
kubectl taint nodes node1 key=value:NoSchedule
```

给节点`node1`加了一个冷屁股。这个冷屁股有一个key`key`，值`value`，冷屁股产生的效果是`NoSchedule`。这就是说不会有Pod调度到`node1`上，除非它有匹配的热脸。

要想删除上面的冷屁股，运行：

```shell script
kubectl taint nodes node1 key:NoSchedule-
```

你在Pod的PodSpec中定义一个热脸。下面的两个热脸都能“贴到”上面`kubectl taint`创建的冷屁股上，只要拥有其中一个热脸就能调度到`node1`：

```yaml
tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"
```

```yaml
tolerations:
- key: "key"
  operator: "Exists"
  effect: "NoSchedule"
```

下面是用例：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "example-key"
    operator: "Exists"
    effect: "NoSchedule"
```

`operator`的默认值是`Equal`。

热脸能贴到冷屁股上的前提条件是，key相同且effect相同，且：

- `operator`是`Exists`（此时不用`value`），或者
- `operator`是`Equal`且`value`相等。

>**注意**：
>
>有俩特例：
>
>`key`为空且operator为`Exists`能够匹配所有key、value和effect，也就是说这张热脸能贴到全世界所有的冷屁股上。
>
>`effect`为空，能够匹配key等于`key`的所有effect。

上面栗子中用的`effect`是`NoSchedule`。你还可以把`effect`改成`PreferNoSchedule`。它是一个“首选”或“软需求”版本的`NoSchedule`——对于没有相应热脸的Pod，系统会*尝试*避免将Pod调度到这些节点上，但并不是一定的。第三种`effect`是`NoExecute`，一会儿再说。

可以在一个节点上设置多个冷屁股，也可以在一个Pod上设置多张热脸。k8s处理多个热恋和冷屁股的时候有点类似于过滤器操作：先从一个节点的所有冷屁股开始，然后忽略掉那些能被Pod热脸贴上的；剩下的冷屁股就会产生它们指定的效果。具体来说，

- 如果剩下了至少一个带有`NoSchedule`效果的冷屁股，那k8s就不会把Pod调度到这个节点上
- 如果剩下的冷屁股没有`NoSchedule`的，但是其中至少有一个效果是`PreferNoSchedule`，k8s会*尝试*不将Pod调度到这个节点上
- 如果剩下的冷屁股至少有一个`NoExecute`，那Pod会被这个节点剔除（前提是已经运行在这个节点上了），并且不会被调度到这个节点上（如果还没有运行在这个节点上）。

比如假设节点的冷屁股是这样的

```shell script
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value2:NoSchedule
```

然后Pod有两张热脸：

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
```

本例中的Pod不会被调度到这个节点上，因为第三个冷屁股没有热脸能贴上去。但是如果在设置冷屁股的时候Pod已经运行在这个节点上了，那就没有影响，因为只有第三个冷屁股没有被贴上。

一般来说，如果节点加了`NoExecute`这种冷屁股，任何没有对应热脸的Pod会被立刻从该节点上剔除。但是一个`NoExecute`效果的热脸可以指定一个`tolerationSeconds`字段，表明在冷屁股添加后，Pod还能在这个节点上停留多久。比如，

```yaml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600
```

这就是说，如果这个Pod已经运行在这个节点上了，然后此时添加了一个对应的冷屁股，那Pod会依然停留在这个节点上，持续3600秒，然后再被剔除。如果热脸在此时间段到期之前被删掉了，Pod是不会被剔除的。

## 用例

热脸和冷屁股，这俩东西可以很灵活地引导Pod*原理*节点或者剔除那些不该继续运行的Pod。其中一些用例包括

- **专用节点**：如果你想让一些节点专门提供给一部分用户，可以给这些节点加一个冷屁股（比如`kubectl taint nodes nodename dedicated=groupName:NoSchedule`）然后给它们的Pod都加上热脸（最简单的实现方式是写一个自定义的[准入控制器](https://v1-18.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/)）。带有热脸的Pod就能使用这些带有冷屁股（专用）的节点，当然也能使用集群中的其他节点。如果你想让这些节点成为它们的专属节点*并且*想确保它们*只能*使用这些节点，那你就需要给这些节点再打上一个和冷屁股类似的节点标签（比如`dedicated=groupName`），然后准入控制器应当给Pod添加对应的节点亲和性，要求Pod只能被调度到带有`dedicated=groupName`标签的节点上。
- **带有特殊硬件的节点**：在一个集群中，一部分节点可能会安装了特殊的硬件（比如GPU），需要让那些用不着这些硬件的Pod远离这些节点，留出空间给那些真正需要的人们。可以给节点加特殊硬件冷屁股（比如`kubectl taint nodes nodename special=true:NoSchedule`或者`kubectl taint nodes nodename special=true:PreferNoSchedule`）然后给那些需要用到这些硬件的Pod加上热脸。同上面专用节点的栗子，应用热脸的最简单的方式依然是自定义[准入控制器](https://v1-18.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/)。比如，建议用[扩展资源](../配置/管理容器资源.md#扩展资源)来表示特殊硬件，用特殊硬件的名字作为节点的冷屁股，然后运行[扩展资源的热脸](https://v1-18.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration)准入控制器。现在，节点有了冷屁股，Pod如果没有热脸是贴不上来的。但如果你提交了一个申请这种扩展资源的Pod，`ExtendedResourceToleration`准入控制器会自动为这些Pod添加对应的热脸，然后Pod就会被调度到带有特殊硬件的节点上了。这样就保证了这些带有特殊硬件的节点成为了专有节点，而且不需要你手动去管理Pod的热脸。
- **基于冷屁股的剔除**：当节点出现问题时，可以基于每个Pod单独配置剔除行为，这玩意我们下一节介绍。

## 基于冷屁股的剔除

**功能状态**：`Kubernetes v1.18 [stable]`

上面提到的`NoExecute`效果的冷屁股，会影响已经运行在节点上的Pod，流程如下

- 如果没有能贴上的热脸，Pod立刻被剔除
- 有对应的热脸，没有设置`tolerationSeconds`，那就会一直留着
- 有对应的热脸，并且设置了`tolerationSeconds`，那就会保留指定的时间

当某些条件出现的时候，节点控制器会给节点自动添加对应的冷屁股。包含了以下内置的冷屁股：

- `node.kubernetes.io/not-ready`：节点未就绪。对应的NodeCondition `Ready`等于“`False`”。
- `node.kubernetes.io/unreachable`：节点控制器无法连接到节点。对应的NodeCondition `Ready`等于“`Unknown`”。
- `node.kubernetes.io/out-of-disk`：节点磁盘没空间了。
- `node.kubernetes.io/memory-pressure`：节点内存不足。
- `node.kubernetes.io/disk-pressure`：节点磁盘空间不多了。
- `node.kubernetes.io/network-unavailable`：节点失联了。
- `node.kubernetes.io/unschedulable`：节点不可调度。
- `node.cloudprovider.kubernetes.io/uninitialized`：如果是通过“外部”云服务商启动的kubelet，会通过这个冷屁股表明该节点不可使用。当cloud-controller-manager的控制器完成该节点的初始化之后，kubelet会删除这个冷屁股。

如果要剔除一个节点，节点控制器或者kubelet会添加相关的冷屁股并且带有`NoExecute`效果。如果是故障情况恢复到正常，kubelet或者节点控制器会删除相关的冷屁股。

>**注意**：control plane会限制给节点添加新的冷屁股的频率。这种频率限制可以当一堆节点同时变成不可达状态时的剔除速度（比如网络出现分裂）。

可以给Pod设置一个`tolerationSeconds`，定义当节点出错或者无响应的时候Pod还能在上面留多久。

比如在出现网络分裂的时候，如果一个应用有很多本地状态，你可能希望它能在这个节点上多呆一会儿，希望网络的分裂可以恢复正常，避免Pod被剔除。可以设置这种热脸：

```yaml
tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 6000
```

>**注意**：
>
>如果你没有明确指定的话，k8s会自动添加`node.kubernetes.io/not-ready`和`node.kubernetes.io/unreachable`，并设置`tolerationSeconds=300`。
>
>这意味着一旦出现对应的状况，Pod会停留5分钟。

[DaemonSet](../业务组件/控制器/DaemonSet.md)的Pod创建的时候会带有以下热脸，并且效果为`NoExecute`，并且还没有`tolerationSeconds`：

- `node.kubernetes.io/unreachable`
- `node.kubernetes.io/not-ready`

这就保证DaemonSet的Pod不会因为这些问题而被节点剔除。

## 根据Condition给节点设置冷屁股

节点生命周期控制器会根据节点的Condition自动创建相应的冷屁股，并带有`NoSchedule`效果。调度器并不会检查节点的Condition，但是会检查节点的冷屁股。这样可以保证节点的Condition不会直接影响节点的调度。用户可以通过添加合适的热脸来选择忽略某些节点问题（通过节点Condition来体现）。

DaemonSet控制器会给所有Pod自动添加以下`NoSchedule`热脸，保证DaemonSet不会出问题。

- `node.kubernetes.io/memory-pressure`
- `node.kubernetes.io/disk-pressure`
- `node.kubernetes.io/out-of-disk`（*仅限非常关键的Pod*）
- `node.kubernetes.io/unschedulable`（从1.10版本开始）
- `node.kubernetes.io/network-unavailable`（*仅限用了主机网络*）

添加了这些热脸可以保证向后兼容性。当然你还可以给DaemonSet添加任意的热脸。

## 下一步……

- 看看如何[处理资源不足](https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/out-of-resource/)
- 看看[Pod优先级](../配置/Pod优先级和抢占.md)