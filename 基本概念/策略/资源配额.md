# 资源配额

当多个用户或者团队共用一个集群的时候，就怕某个团队资源用的太多，其他团队嚷嚷没资源，两边打的不可开交，运维直接跑路。

资源配额就是出来解决这个问题的。

资源配额，由`ResourceQuota`对象来定义，用来限制每个命名空间下可用资源总量。它可以根据类型来限制一个命名空间下可以创建对象数量的上限，以及计算资源的总量。

资源配额是这样婶儿的：

- 不同团队工作在不同的命名空间中。目前这句话没什么约束，未来可能会通过ACL强制实现。
- 管理员给每个命名空间创建一个`ResourceQuota`。
- 用户在自己的命名空间中创建资源（Pod、Service等），配额系统会跟踪它们的用量，确保它们不会超过`ResourceQuota`中设置的硬性指标。
- 如果创建或更新的资源违反了配额约束，请求则以HTTP`403 FORBIDDEN`告终，并告诉你该操作会违反怎样的约束。
- 如果在命名空间中给`cpu`或者`memory`开启了配额，用户就必须要指定它们的请求（request）或上限（limit）值；否则配额系统会拒绝创建Pod。小贴士：可以用`LimitRanger`准入控制器（admission controller）为没有设定资源需求的Pod注入默认值。

关于上面说的最后一条，[这里](https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)有一个详细的栗子。

`ResourceQuota`对象的名字必须是[有效的DNS子域名](../概要/Kubernetes对象/对象的名字和ID.md#DNS子域名)。

具体栗子：

- 比如一个总计有32G16C的集群中，给A队分20G10C，B队10G4C，留下2G2C以备后用。
- 给“testing”命名空间提供1C1G。“production”命名空间可以不加限制随便用。

如果集群容量少于命名空间配额总数，就会出现资源争用。这种情况下就是先到先得，后来的哭死在十字街头，哎？怎么还押韵了。

资源的争用以及quota都不会影响到先前已经创建好的资源。

## 开启资源配额

在很多k8s发行版中都是默认打开了资源配额。如果没有的话，可以把`ResourceQuota`加到apiserver的`--enable-admission-plugins=`。

如果一个命名空间中有了一个`ResourceQuota`，那就会强制执行。

## 计算资源配额

可以限制一个命名空间中可以请求的[计算资源](../配置/管理容器资源.md)总量。

支持的资源类型包括：

**资源名**|描述
-|-
`limits.cpu`|所有处于非结束状态的Pod，CPU上限（limit）总和不能超过这个值。
`limits.memory`|所有处于非结束状态的Pod，内存上限（limit）总和不能超过这个值。
`request.cpu`|所有处于非结束状态的Pod，CPU请求（request）总和不能超过这个值。
`request.memory`|所有处于非结束状态的Pod，内存请求（request）总和不能超过这个值。

### 对于扩展资源的资源配额

除了上面提到的这几种资源，从1.10版本开始，增加了对[扩展资源](../配置/管理容器资源.md#扩展资源)的资源配额。

扩展资源不允许过量使用（overcommit），对于一个扩展资源的配额，同时设置`requests`和`limits`是没什么卵用的。所以对于扩展资源，目前只能用`requests.`这种配额前缀。

就拿GPU来说吧，如果资源名为`nvidia.com/gpu`，然后你想限制一个命名空间中GPU请求总数上限为4，那就这么写：

- `requests.nvidia.com/gpu: 4`

详见[查看和设置配额](#查看和设置配额)。

## 存储资源配额

你还可以限制一个命名空间中[存储资源](../存储/持久卷（Persistent%20Volume）.md)的请求数量上限。

而且你还能基于每个StorageClass来限制存储资源的使用。

**资源名**|**描述**
-|-
`requests.storage`|所有PVC请求的存储总量不能超过这个值。
`persistentvolumeclaims`|命名空间下可以创建的[PVC](../存储/持久卷（Persistent%20Volume）.md#PVC)数量上限。
`<storage-class-name>.storageclass.storage.k8s.io/requests.storage`|所用用到这个StorageClass的PVC，存储请求总量不能超过这个值。
`<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims`|所有用到这个StorageClass的PVC，[PVC](../存储/持久卷（Persistent%20Volume）.md#PVC)的数量不能超过这个值。

比如有个人想把`gold`和`bronze`这两个StorageClass的存储配额区分一下，他可以这么写：

- `gold.storageclass.storage.k8s.io/requests.storage: 500Gi`
- `bronze.storageclass.storage.k8s.io/requests.storage: 100Gi`

在1.8版本中，配额系统为本地临时存储提供了alpha版本的支持：

**资源名**|**描述**
-|-
`requests.ephemeral-storage`|对于命名空间中的所有Pod，本地临时存储的请求总量不能超过这个值。
`limits.ephemeral-storage`|对于命名空间中的所有Pod，本地临时存储的上限总和不能超过这个值。

## 对象计数配额

从1.9版本开始，对于所有基于命名空间的标准资源类型，可以使用下面的语法来进行配额管理：

- `count/<resource>.<group>`

下面是一些用户可能希望进行配额管理的资源：

- `count/persistentvolumeclaims`
- `count/services`
- `count/secrets`
- `count/configmaps`
- `count/replicationcontrollers`
- `count/deployments.apps`
- `count/replicasets.apps`
- `count/statefulsets.apps`
- `count/jobs.batch`
- `count/cronjobs.batch`
- `count/deployments.extensions`

在1.15版本中增加了对自定义资源的类似语法支持。比如要给`example.com`API组下的自定义资源`widgets`创建一个配额，那就是`count/widgets.example.com`。

如果使用`count/*`资源配额，只要对象存在于服务器中，那就会收到配额的管理。这种配额通常有助于保护存储资源不被耗尽。比如Secret的大小比较大的话，你肯定想限制一下它们的数量。如果集群中的Secret数量超大，可能会导致集群根本起不来！再比如你可能会对Job做配额管理，避免那些没配好的CronJob创建了一堆Job导致无法继续服务。

在1.9版本之前，可以在一组资源上做一般性的对象计数配额。此外对于每种资源可以根据具体类型进一步做好配额限制。

支持以下类型：

**资源名**|**描述**
-|-
`configmap`|命名空间下可以存在的ConfigMap的总数。
`persistentvolumeclaims`|命名空间下可以存在的[PVC](../存储/持久卷（Persistent%20Volume）.md)的总数。
`pods`|命名空间下可以存在的非终结（non-terminal）状态下的Pod总数。如果Pod的`.status.phase in (Failed, Succeeded)`，那它就是终结状态。
`replicationcontrollers`|命名空间下可以存在的ReplicationController的总数。
`resourcequotas`|命名空间下可以存在的[ResourceQuota](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#resourcequota)总数。
`services`|命名空间下可以存在的Service总数。
`services.loadbalancers`|命名空间下可以存在的LoadBalancer类型的Service总数。
`services.nodeports`|命名空间下可以存在的NodePort类型的Service总数。
`secrets`|命名空间下可以存在的Secret总数。

比如吧，`pods`配额强制限制一个命名空间下非终结状态下的`Pod`总数。比如你要是担心有人创建很多小Pod，耗尽IP，那就在命名空间下建一个`pods`配额。

## 配额作用域

每个配额都有对应的作用域（scope）。配额在多种作用域的交集下才能发挥它的作用。

当配额设置了作用域后，它会根据作用域来限制对应资源的数量。如果指定的资源不在允许的范围内，那就报校验错误。

**作用域**|**描述**
-|-
`Terminating`|匹配`.spec.activeDeadlineSeconds >= 0`的Pod。
`NotTerminating`|匹配`.spec.activeDeadlineSeconds is nil`的Pod。
`BestEffort`|匹配那些鞠躬尽瘁死而后已的Pod。
`NotBestEffort`|匹配那些吊儿郎当不思进取的Pod。

`BestEffort`作用域会限制配额监视以下资源：`pods`。

`Terminating`、`NotTerminating`和`NotBestEffort`作用域会限制配额监视以下资源：

- `cpu`
- `limits.cpu`
- `limits.memory`
- `memory`
- `pods`
- `requests.cpu`
- `requests.memory`

### 每个PriorityClass的资源配额

**功能状态**: `Kubernetes v1.12 [beta]`

Pod可以按照[优先级](../配置/Pod优先级与抢占.md)来创建。可以基于Pod的优先级来控制其对系统资源的使用，需要在配额定义中设置`scopeSelector`字段。

只有当`scopeSelector`选中了对应的Pod，才会对其应用配额。

这里创建了一个配额对象，将它匹配到了指定优先级的Pod。流程如下：

- 集群中的Pod有三种优先级，“low”、“medium”和“high”。
- 每个优先级创建一个配额对象。

将下面的YAML保存到`quota.yaml`文件。

```yaml
apiVersion: v1
kind: List
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-high
  spec:
    hard:
      cpu: "1000"
      memory: 200Gi
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["high"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-medium
  spec:
    hard:
      cpu: "10"
      memory: 20Gi
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["medium"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-low
  spec:
    hard:
      cpu: "5"
      memory: 10Gi
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["low"]
```

用`kubectl create`命令来创建。

```shell script
kubectl create -f ./quota.yml
```

```text
resourcequota/pods-high created
resourcequota/pods-medium created
resourcequota/pods-low created
```

调用`kubectl describe quota`命令检查`Used`配额为`0`。

```shell script
kubectl describe quota
```

```text
Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     1k
memory      0     200Gi
pods        0     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
```

创建一个“high”优先级的Pod。将下面的YAML保存到`high-priority-pod.yaml`文件。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority
spec:
  containers:
  - name: high-priority
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "10Gi"
        cpu: "500m"
      limits:
        memory: "10Gi"
        cpu: "500m"
  priorityClassName: high
```

执行`kubectl create`。

```shell script
kubectl create -f ./high-priority-pod.yml
```

检查“high”优先级配额的“Used”，`pods-high`，发生了变化，另外两个还没有变化。

```shell script
kubectl describe quota
```

```text
Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         500m  1k
memory      10Gi  200Gi
pods        1     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
```

`scopeSelector`的`operator`字段支持以下值：

- `In`
- `NotIn`
- `Exist`
- `DoesNotExist`

## 请求值和上限值

分配计算资源的时候，每个容器都可以设置对CPU或内存的请求和上限值。配额可以对它们进行分别管理。

如果配额中设置了`requests.cpu`或`requests.memory`，那就需要每个容器创建的时候都要指定对应资源的请求值。如果配额中设置了`limits.cpu`或`limits.memory`，那就需要每个容器在创建的时候都要指定对应资源的上限值。

## 查看和设置配额

kubectl支持创建、更新和查看配额：

```shell script
kubectl create namespace myspace
```

```shell script
cat <<EOF > compute-resources.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    requests.nvidia.com/gpu: 4
EOF
```

```shell script
kubectl create -f ./compute-resources.yaml --namespace=myspace
```

```shell script
cat <<EOF > object-counts.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    configmaps: "10"
    persistentvolumeclaims: "4"
    pods: "4"
    replicationcontrollers: "20"
    secrets: "10"
    services: "10"
    services.loadbalancers: "2"
EOF
```

```shell script
kubectl create -f ./object-counts.yaml --namespace=myspace
```

```shell script
kubectl get quota --namespace=myspace
```

```text
NAME                    AGE
compute-resources       30s
object-counts           32s
```

```shell script
kubectl describe quota compute-resources --namespace=myspace
```

```text
Name:                    compute-resources
Namespace:               myspace
Resource                 Used  Hard
--------                 ----  ----
limits.cpu               0     2
limits.memory            0     2Gi
requests.cpu             0     1
requests.memory          0     1Gi
requests.nvidia.com/gpu  0     4
```

```shell script
kubectl describe quota object-counts --namespace=myspace
```

```text
Name:                   object-counts
Namespace:              myspace
Resource                Used    Hard
--------                ----    ----
configmaps              0       10
persistentvolumeclaims  0       4
pods                    0       4
replicationcontrollers  0       20
secrets                 1       10
services                0       10
services.loadbalancers  0       2
```

对于所有标准的基于命名空间的资源，kubectl为对象计数配额提供`count/<resource>.<group>`语法：

```shell script
kubectl create namespace myspace
```

```shell script
kubectl create quota test --hard=count/deployments.extensions=2,count/replicasets.extensions=4,count/pods=3,count/secrets=4 --namespace=myspace
```

```shell script
kubectl create deployment nginx --image=nginx --namespace=myspace --replicas=2
```

```shell script
kubectl describe quota --namespace=myspace
```

```text
Name:                         test
Namespace:                    myspace
Resource                      Used  Hard
--------                      ----  ----ber
count/deployments.extensions  1     2
count/pods                    2     3
count/replicasets.extensions  1     4
count/secrets                 1     4
```

## 配额与集群容量

`ResourceQuotas`与集群容量是相互独立的。它们是用绝对单位来表示的。因此当你给集群添加了节点，*并不会*自动让每个命名空间可以使用更多的资源。

有的时候还需要更加复杂的策略，比如：

- 将集群资源按比例划分给每个团队。
- 允许每个租户按需增加资源使用，还需要较为宽松的上限，避免偶然的资源耗尽。
- 关注某个命名空间的需求，添加节点并增加配额。

这些策略都可以考虑`ResourceQuotas`作为实现方案的一部分，写一个“控制器”来监控配额的使用量，根据其他的信号来调整每个命名空间下配额的限制。

注意资源配额将整体的集群资源拆分开了，但是不会对节点做什么限制：多个命名空间下的Pod还是可以运行在同一个节点的。

## 限制默认的PriorityClass

我们可能希望当且仅当一个对应的配额对象存在的情况下，指定优先级，比如“cluster-services”的Pod才能允许存在于这个命名空间下。

有了这种机制，操作员可以限制命名空间下的某些高优先级类型的数量，默认情况下并不是每个命名空间都能够使用这些优先级类型。

要想强制实现这个机制，需要将apiserver的`--admission-control-config-file`指向下面两个文件之一：

>**apiserver.config.k8s.io/v1**
>
>```yaml
>apiVersion: apiserver.config.k8s.io/v1
>kind: AdmissionConfiguration
>plugins:
>- name: "ResourceQuota"
>  configuration:
>    apiVersion: apiserver.config.k8s.io/v1
>    kind: ResourceQuotaConfiguration
>    limitedResources:
>    - resource: pods
>      matchScopes:
>      - scopeName: PriorityClass
>        operator: In
>        values: ["cluster-services"]
>```
>
>**apiserver.k8s.io/v1alpha1**
>
>```yaml
># Deprecated in v1.17 in favor of apiserver.config.k8s.io/v1
>apiVersion: apiserver.k8s.io/v1alpha1
>kind: AdmissionConfiguration
>plugins:
>- name: "ResourceQuota"
>  configuration:
>    # Deprecated in v1.17 in favor of apiserver.config.k8s.io/v1, ResourceQuotaConfiguration
>    apiVersion: resourcequota.admission.k8s.io/v1beta1
>    kind: Configuration
>    limitedResources:
>    - resource: pods
>      matchScopes:
>      - scopeName: PriorityClass
>        operator: In
>        values: ["cluster-services"]
>```

现在好了，“cluster-services”类型的Pod只能存在于配额对象跟`scopeSelector`匹配的命名空间中了。比如：

```yaml
    scopeSelector:
      matchExpressions:
      - scopeName: PriorityClass
        operator: In
        values: ["cluster-services"]
```

更多小知识请看[受限资源](https://github.com/kubernetes/kubernetes/pull/36765)和[优先级类型的配额支持](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-priority-resourcequota.md)。

## 栗子

跪下学一个[如何使用资源配额的详细的栗子](https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/quota-api-object/)。

## 下一步……

- 看看[资源配额的设计文档](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/admission_control_resource_quota.md)。