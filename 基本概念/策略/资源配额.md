# 资源配额

当多个用户或者团队共用一个集群的时候，就怕某个团队资源用的太多，其他团队嚷嚷没资源，两边打的不可开交，运维直接跑路。

资源配额就是出来解决这个问题的。

资源配额，由`ResourceQuota`对象来定义，用来限制每个命名空间下可用资源总量。它可以根据类型来限制一个命名空间下可以创建对象数量的上限，以及计算资源的总量。

资源配额是这样婶儿的：

- 不同团队工作在不同的命名空间中。目前这句话没什么约束，未来可能会通过ACL强制实现。
- 管理员给每个命名空间创建一个`ResourceQuota`。
- 用户在自己的命名空间中创建资源（Pod、Service等），配额系统会跟踪它们的用量，确保它们不会超过`ResourceQuota`中设置的硬性指标。
- 如果创建或更新的资源违反了配额约束，请求则以HTTP`403 FORBIDDEN`告终，并告诉你该操作会违反怎样的约束。
- 如果在命名空间中给`cpu`或者`memory`开启了配额，用户就必须要指定它们的请求（request）或上限（limit）值；否则配额系统会拒绝创建Pod。小贴士：可以用`LimitRanger`准入控制器（admission controller）为没有设定资源需求的Pod注入默认值。

关于上面说的最后一条，[这里](https://v1-18.docs.kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)有一个详细的栗子。

`ResourceQuota`对象的名字必须是[有效的DNS子域名](../概要/Kubernetes对象/对象的名字和ID.md#DNS子域名)。

具体栗子：

- 比如一个总计有32G16C的集群中，给A队分20G10C，B队10G4C，留下2G2C以备后用。
- 给“testing”命名空间提供1C1G。“production”命名空间可以不加限制随便用。

如果集群容量少于命名空间配额总数，就会出现资源争用。这种情况下就是先到先得，后来的哭死在十字街头，哎？怎么还押韵了。

资源的争用以及quota都不会影响到先前已经创建好的资源。

## 开启资源配额

在很多k8s发行版中都是默认打开了资源配额。如果没有的话，可以把`ResourceQuota`加到apiserver的`--enable-admission-plugins=`。

如果一个命名空间中有了一个`ResourceQuota`，那就会强制执行。

## 计算资源配额

可以限制一个命名空间中可以请求的[计算资源](../配置/管理容器资源.md)总量。

支持的资源类型包括：

**资源名**|描述
-|-
`limits.cpu`|所有处于非结束状态的Pod，CPU上限（limit）总和不能超过这个值。
`limits.memory`|所有处于非结束状态的Pod，内存上限（limit）总和不能超过这个值。
`request.cpu`|所有处于非结束状态的Pod，CPU请求（request）总和不能超过这个值。
`request.memory`|所有处于非结束状态的Pod，内存请求（request）总和不能超过这个值。

### 对于扩展资源的资源配额

除了上面提到的这几种资源，从1.10版本开始，增加了对[扩展资源](../配置/管理容器资源.md#扩展资源)的资源配额。

扩展资源不允许过量使用（overcommit），对于一个扩展资源的配额，同时设置`requests`和`limits`是没什么卵用的。所以对于扩展资源，目前只能用`requests.`这种配额前缀。

就拿GPU来说吧，如果资源名为`nvidia.com/gpu`，然后你想限制一个命名空间中GPU请求总数上限为4，那就这么写：

- `requests.nvidia.com/gpu: 4`

详见[查看和设置配额](#查看和设置配额)。

## 存储资源配额

你还可以限制一个命名空间中[存储资源](../存储/持久卷（Persistent%20Volume）.md)的请求数量上限。

而且你还能基于每个StorageClass来限制存储资源的使用。

**资源名**|**描述**
-|-
`requests.storage`|所有PVC请求的存储总量不能超过这个值。
`persistentvolumeclaims`|命名空间下可以创建的[PVC](../存储/持久卷（Persistent%20Volume）.md#PVC)数量上限。
`<storage-class-name>.storageclass.storage.k8s.io/requests.storage`|所用用到这个StorageClass的PVC，存储请求总量不能超过这个值。
`<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims`|所有用到这个StorageClass的PVC，[PVC](../存储/持久卷（Persistent%20Volume）.md#PVC)的数量不能超过这个值。

比如有个人想把`gold`和`bronze`这两个StorageClass的存储配额区分一下，他可以这么写：

- `gold.storageclass.storage.k8s.io/requests.storage: 500Gi`
- `bronze.storageclass.storage.k8s.io/requests.storage: 100Gi`

在1.8版本中，配额系统为本地临时存储提供了alpha版本的支持：

**资源名**|**描述**
-|-
`requests.ephemeral-storage`|对于命名空间中的所有Pod，本地临时存储的请求总量不能超过这个值。
`limits.ephemeral-storage`|对于命名空间中的所有Pod，本地临时存储的上限总和不能超过这个值。

## 对象计数配额

从1.9版本开始，对于所有基于命名空间的标准资源类型，可以使用下面的语法来进行配额管理：

- `count/<resource>.<group>`

下面是一些用户可能希望进行配额管理的资源：

- `count/persistentvolumeclaims`
- `count/services`
- `count/secrets`
- `count/configmaps`
- `count/replicationcontrollers`
- `count/deployments.apps`
- `count/replicasets.apps`
- `count/statefulsets.apps`
- `count/jobs.batch`
- `count/cronjobs.batch`
- `count/deployments.extensions`

## 查看和设置配额