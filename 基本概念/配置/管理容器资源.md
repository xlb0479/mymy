# 管理容器资源

当你定义了一个[Pod](../业务组件/泡德（Pod）/Pod.md)，你还可以定义每个[容器](../概要/Kubernetes是啥？.md)需要用多少资源。最常见的就是CPU和内存了（RAM）；当然也有其他的。

当你给Pod中的容器设置了资源*请求（request）*，调度器会根据这个值来判断Pod可以被调度到哪个节点上。当你给容器设置了资源*上限（limit）*，kubelet会强制施加限制，保证容器使用的资源不会超过你设定的上限。同时，kubelet还会保证留下至少资源*请求*的数量，保证容器需要的最少资源。

## 请求（request）和限制（limit）

如果Pod所在的节点有足够的资源，那就能够（允许）让容器使用资源超过它的`request`。但是容器使用的资源不能超过`limit`。

比如你设置了容器的`memory`请求值为256MiB，它所在的Pod被调度到了一个拥有8GiB内存的节点上，并且该节点没有其他Pod，那这个容器就可以使用更多的RAM。

如果你为容器设置了`memory`的上限为`4GiB，kubelet（以及[容器运行时](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)）会强制执行这个限制。这个运行时会限制容器使用的资源不超过设定的上限。比如：进程中的容器想要使用超过上限的内存，系统内核会停掉尝试分配内存的进程，并给出一个内存不足（OOM）的错误信息。

限制措施可以实现成反应式的（当系统发现非法操作时进行干预）或者是强制型的（系统阻止容器超过上限）。不同的运行时对同一种限制也会有不同的实现方法。

## 资源类型

*CPU*和*内存*都是*资源类型*。每个资源类型都有一个基本单位。CPU代表着计算处理，使用[k8s CPU](#CPU的含义)单位。内存使用字节单位。如果你用的是v1.14或以上版本，还可以设置*大页内存（huge page）*资源。大页内存是Linux的特性，节点内核可以分配的内存块要远大于默认的页大小。

比如一个系统默认页大小是4KiB，你可以设置一个上限，`hugepages-2Mi: 80Mi`。如果容器尝试分配超过40个2MiB的大页（总计80MiB），分配就会失败。

>**注意**：不能过度使用`hugepages-*`资源。这种资源跟`memory`和`cpu`不一样哦。

CPU和内存统称为*计算资源（compute resource）*，或者直接叫*资源（resource）*。计算资源是可以根据数量度量进行请求、分配和使用的。它们跟[API资源](../概要/Kubernetes%20API.md)不同。API资源，比如Pod和[Service](../Service，负载均衡，网络/Service.md)都是对象，可以通过apiserver进行读取和修改。

## Pod和容器的资源请求与限制

一个Pod中的每个容器都可以设置一个或多个：

- `spec.containers[].resources.limits.cpu`
- `spec.containers[].resources.limits.memory`
- `spec.containers[].resources.limits.hugepages-<size>`
- `spec.containers[].resources.requests.cpu`
- `spec.containers[].resources.requests.memory`
- `spec.containers[].resources.requests.hugepages-<size>`

尽管请求和限制只能基于特定容器进行设定，获取Pod级别的资源请求和限制也是很方便的。对于某个资源类型，*Pod资源请求/限制*值就是其中每个容器的对该资源的请求/限制总和。

## k8s中的资源单位

### CPU的含义

CPU的请求和限制用*cpu*单位进行度量。在k8s中，一个CPU，就等于云服务商的**1个vCPU或1个内核**，对于裸金属的Intel处理器来说就是**1个超线程**。

可以按小数进行请求。如果容器设定的`spec.containers[].resources.requests.cpu`值为`0.5`，就保证可以请求1个CPU的一半。同时，`0.1`还等于`100m`，读作“100个millicpu”，有些人也读作“100个millicore”，都是一样的意思。如果请求值带了小数，比如`0.1`，会被CPU转换成`100m`，不允许比`1m`更小的精度。所以，更倾向于使用`100m`这种格式。

CPU总是按照绝对数量进行请求的，而不会是相对数量；0.1在单核、双核，甚至48核的机器上代表的数量都是一样的。

### 内存的含义

`memory`的请求和限制用字节进行度量。可以用普通的整数或者定点数来表示内存，并加上这些后缀之一：E、P、T、G、M、K。还可以使用对应的二次方形式：Ei、Pi、Ti、Gi、Mi、Ki。比如下面这些基本上都是差不多一样的值：

```text
128974848, 129e6, 129M, 123Mi
```

这里有个栗子。下面这个Pod有两个容器。每个容器都请求了0.25个CPU和64MiB（2<sup>26</sup>字节）的内存。每个容器资源上限都是0.5个CPU和128MiB内存。那我们就可以说，这个Pod请求了0.5个CPU和128MiB内存，上限为1个CPU和256MiB内存。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    env: # 我去，这里空着？？？
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
```

## 带资源请求的Pod是如何调度的

当你创建了一个Pod，k8s的调度器就要为这个Pod选择一个合适的节点。对于每种资源，每个节点都有它的最大容量值：它可以为Pod提供的CPU和内存数量。调度器能够确保对于每种资源，容器的请求值总和要小于节点的容量值。需要注意的是，尽管节点上真实使用的内存或CPU资源可能非常低，但如果容量检查不通过，调度器依然不会对Pod进行调度。这就保证了在之后可能出现资源使用率上升的情况下不会出现资源不足的问题，比如每日的请求高峰期。

## 带资源限制的Pod是怎么运行的

当kubelet启动了Pod中的一个容器，它会将CPU和内存的限制传给容器运行时。

对于Docker：

- `spec.containers[].resources.requests.cpu`会被转换成它的核数，可能是小数，然后乘以1024。得到的这个值和2进行比较，取较大值，作为`docker run`命令的[`--cpu-shares`](https://docs.docker.com/engine/reference/run/#cpu-share-constraint)选项。
- `spec.containers[].resources.limits.cpu`会转换成millicore值，然后乘以100。结果值就代表每100毫秒，容器可以使用的CPU时间上限。在这个时间内，容器可用的CPU时间无法超过这个上限。

>**注意**：默认的周期配额就是100毫秒。CPU最小周期配额为1毫秒。

- `spec.containers[].resources.limits.memory`会转换成一个整数，作为`docker run`命令的[`--memory`](https://docs.docker.com/engine/reference/run/#/user-memory-constraints)选项的值。

如果容器超出了内存限制就有可能被终结者拿着冒蓝火的加特林突突死。如果容器可以重启，那kubelet就重启它，这跟其他类型的错误处理方式是一样的。

如果容器内存使用超出了请求（request），当节点内存不足的时候，这个Pod就有可能被踢掉。

容器可能会，也可能不会，在一段时间内允许超出CPU限制。但是CPU超量使用不会导致容器被突突死。

要想判断一个容器是否会因为资源限制导致无法调度活着被突突死，参见[排错](#排错)一节。

### 监控计算和内存资源的使用

Pod的资源使用情况会作为Pod的status的一部分报告出来。

如果集群中有可用的[监控工具](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/)，Pod的资源占用情况既可以通过[Metrics API](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api)获取，也可以通过你的监控工具来获取。

## 本地临时存储

**功能状态**：`Kubernetes v1.10 [beta]`

节点都是有本地临时存储的，基于本地的可写设备，或者有时候是RAM。“临时”意味着不对长期持久化做任何保证。

Pod可以使用临时存储进行初始化空间、缓存或者日志。kubelet可以让Pod挂载[`emptyDir`](../存储/数据卷.md#emptydir)[数据卷](../存储/数据卷.md)来使用临时存储。

kubelet还会用这种存储来保存[节点级别的容器日志](https://kubernetes.io/docs/concepts/cluster-administration/logging/#logging-at-the-node-level)，容器镜像，以及运行时容器的可写层。

>**小心**：如果节点挂了，临时存储中的数据就都没了。你的应用不能最临时存储的性能SLA（比如磁盘IOPS）做任何期待。

作为一个beta版本的功能，k8s可以允许你跟踪、保留和限制临时存储的用量。

### 配置本地临时存储

k8s支持两种配置本地临时存储的方法：

>**单个文件系统**：
>
>这种情况下，你把所有类型的临时存储数据（`emptyDir`数据卷、可写层、容器镜像、日志）都放到了一个文件系统中。这种最高效的kebelet配置方式就意味着整个文件系统都是用来给k8s（kubelet）数据提供支持的。
>
>kubelet还会写入节点级别的容器日志，把它们当做类似本地临时存储的东西。
>
>kubelet将日志写入配置好的日志目录中（默认是`/var/log`）；对于其他数据的本地存储，也有一个根目录（默认是`/var/lib/kubelet`）。
>
>一般来说`/var/lib/kubelet`和`/var/log`都是在根文件系统上的，kubelet设计的时候也是这么假设的。
>
>当然，你的节点可以有多个文件系统，这个都是你自己看着弄就行。

>**两个文件系统**：
>
>在节点上，你有一个文件系统用于为Pod做临时数据存储：日志、以及`emptyDir`数据卷。可以将这个文件系统用于其他数据（比如和k8s无关的系统日志）；甚至可以用作根文件系统。
>
>kubelet会将节点级别的容器日志写入第一个文件系统，对它的用法也类似于本地临时存储一样。
>
>然后你还用了另一个文件系统，基于不同的逻辑存储设备。此时，你将kubelet保存容器镜像层以及可写层的地方配置到了这第二个文件系统上。
>
>第一个文件系统不包含任何镜像层或者可写层。
>
>当然，你的节点可以有多个文件系统，这个都是你自己看着弄就行。

kubelet可以度量出本地存储的使用情况。这需要：

- 开启`LocalStorageCapacityIsolation`[特性门](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/)（默认已经打开），还有
- 你在安装这个节点的时候已经加好了本地临时存储的相关配置。

如果你的配置不一样，kubelet就无法提供关于本地临时存储的资源限制。

>**注意**：kubelet会将emptyDir数据卷`tmpfs`计入容器内存占用，而不是本地临时存储。

### 设置本地临时存储的请求和限制

可以使用*ephemeral-storage*来管理本地临时存储。可以定义一个或多个：

- `spec.containers[].resources.limits.ephemeral-storage`
- `spec.containers[].resources.requests.ephemeral-storage`

`ephemeral-storage`的单位是字节。可以用普通整数或者定点数来表达，加上这些后缀：E、P、T、G、M、K。还可以用对应的二次幂形式：Ei、Pi、Ti、Gi、Mi、Ki。比如下面这几个值表达的值基本上都是一样的：

```text
128974848, 129e6, 129M, 123Mi
```

在下面的栗子中，Pod有俩容器。每个容器都请求了2GiB的本地临时存储。上限都是4GiB。因此，这个Pod总共请求了4GiB的本地临时存储，上限是8GiB。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        ephemeral-storage: "2Gi"
      limits:
        ephemeral-storage: "4Gi"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        ephemeral-storage: "2Gi"
      limits:
        ephemeral-storage: "4Gi"
```

### 带临时存储请求的Pod是怎么调度的

当你创建了一个Pod，k8s的调度器就要给这个Pod选择一个合适的节点来运行。每个节点都有它可以提供的本地临时存储上限。详见[节点allocatable](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)。

调度器会保证被调度的容器的资源请求总和不大于节点的容量。

### 临时存储的消耗管理

如果kubelet将本地临时存储作为一种资源来管理，kubelet会度量以下几方面的东西：

- `emptyDir`数据卷，除了*tmpfs*的`emptyDir`数据卷
- 保存节点级别日志的目录
- 容器的可写层

如果Pod超出了临时存储的用量限制，kubelet就会发出一个踢人信号，触发节点剔除。

对于容器级别的隔离，如果一个容器的可写层以及日志占用超过了存储限制，kubelet同样要将Pod标记为剔除状态。

对于Pod级别的隔离，kubelet要统计Pod中所有容器的用量限制然后求和。此时，如果对Pod中所有容器求和之后，加上Pod的`emptyDir`数据卷的用量，超过的Pod的存储限制，kubelet同样要将Pod标记为剔除状态。

>**小心**：
>
>如果kubelet不度量本地临时存储的用量，那Pod就不会因为容量使用超限被剔除。
>
>但是如果容器可写层、节点日志、以及`emptyDir`数据卷在文件系统上的可用空间不足，节点会给自己加上[冷屁股（taint）](../调度和驱逐/热脸和冷屁股.md)，表明自己存储空间不足，这就会导致节点上的Pod如果没有热脸（tolerate ）能贴到这个冷屁股上，就都会被剔除。
>
>见本地临时存储相关[配置](#配置本地临时存储)。

kubelet有两种方法来度量Pod的存储：

>**周期性的扫描**
>
>kubelet执行常规的调度检查，扫描每一个`emptyDir`数据卷、容器日志目录以及容器可写层。
>
>这个扫描会度量它们用了多少空间。
>
>>**注意**：
>>
>>在这种模式下，kubelet不会跟踪那些文件已经被删除的打开的文件描述符。
>>如果你（或者容器）在一个`emptyDir`数据卷中创建了一个文件，然后某个路人甲打开了这个文件，此时你又将文件删除掉了，这个被删除文件的inode依然存在，直到你关闭了文件，但是kubelet不会将其计入空间使用量中。

>**文件系统项目配额**
>
>**功能状态**：`Kubernetes v1.15 [alpha]`
>
>项目配额是操作系统层面上用来管理文件系统存储的功能。对于k8s，你可以开启项目配额来监控存储的使用情况。要确保节点上用于支持`emptyDir`数据卷的文件系统能够提供项目配额功能。比如，XFS和ext4fs就可以支持项目配额。
>
>>**注意**：项目配额是让你监控存储使用的；不是用来施加强制限制的。
>
>k8s使用从`1048567`开始的项目ID。使用的ID会注册在`/etc/projects`和`/etc/projid`中。如果这个范围内的ID被用于系统的其他部分，必须要将它们注册到`/etc/projects`和`/etc/projid`中，这样k8s就不会用它们了。
>
>配额比目录扫描的方式要更快更精确。当一个目录指派给一个项目，目录下所有文件的创建都是在这个项目中进行创建，内核只需要跟踪该项目下的文件只用了多少文件块。
>
>当文件被删除后，但同时还有打开的文件描述符，它就依然占用空间。配额可以精确跟踪到空间的使用情况，而目录扫描就会忽略那些被删除的文件所占用的空间。
>
>如果你想使用项目配额，你需要：
>
>- 在kubelet的配置中开启`LocalStorageCapacityIsolationFSQuotaMonitoring=true`特性门。
>- 确保根文件系统（或者可选的运行时文件系统）已经开启了项目配额。所有的XFS文件系统都可以支持项目配额，对于ext4文件系统，需要在文件系统挂载之前开启项目配额跟踪的功能。
>
>```shell script
># 对于ext4，如果/dev/block-device还没有挂载
>sudo tune2fs -O project -Q prjquota /dev/block-device
>```
>
>- 确保根文件系统（或者可选的运行时文件系统）挂载的时候已经开启了项目配额。对于XFS和ext4fs，对应的挂载选项为`prjquota`。

## 扩展资源

扩展资源（Extended Resource）是位于`kubernetes.io`域外部的完全限定的资源名。它们可以让集群操作者发布，然后让用户使用非k8s内置的资源。

要使用扩展资源，拢共分两步。第一步，集群操作者要发布扩展资源。第二步，用户在Pod中请求扩展资源。第三步，把冰箱门儿带上。

### 管理扩展资源

#### 节点级别的扩展资源

节点级别的扩展资源是绑定到节点上的。

##### 基于设备插件进行管理的资源

去看[设备插件（Device Plugin）](../计算，存储与网络拓展/设备插件.md)，了解如何在每个节点上发布基于设备插件管理的资源。

##### 其他资源

要发布一个新的节点级别的扩展资源，集群操作者可以提交一个`PATCH`方法的HTTP请求到apiserver，在`status.capacity`中声明一个节点可用的资源数量。然后，该节点的`status.capacity`就会包含一个新的资源。kubelet会自动的异步对新资源的`status.allocatable`进行更新。要注意因为调度器需要用到节点的`status.allocatable`来判断Pod是否可以被调度到这个节点上，所以在该请求发送给节点，到第一个需要该资源的Pod被调度到这个节点，在这个时间中会有一定的延迟。

**栗子：**

下面的栗子用`curl`发送了一个HTTP请求，在`k8s-node-1`这个节点上发布了五个“example.com/foo”资源，它的master是`k8s-master`。

```shell script
curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]' \
http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
```

>**注意**：在上面的栗子中，`~1`是patch路径中对`/`字符的编码。JSON-Patch中的操作路径会被解释成JSON-Pointer。详见[IETF RFC 6901的第3小节](https://tools.ietf.org/html/rfc6901#section-3)。

#### 集群级别的扩展资源

集群级别的扩展资源自然不是绑定到节点上的了。它们一般是由调度器扩展（scheduler extender）来管理的，它负责资源的使用和配额。

可以在[调度器策略配置](https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/scheduler/api/v1/types.go#L31)中声明调度器扩展所管理的扩展资源。

**栗子：**

下面是一个调度器策略的配置，其中，“example.com/foo”这个集群级别的扩展资源就是由调度器扩展来管理的。

- 只有当Pod申请了“example.com/foo”，调度器才会将这个Pod发送给调度器扩展。
- `ignoredByScheduler`字段表明调度器不会检查“example.com/foo”资源在`PodFitsResources`中的期望。

```json
{
  "kind": "Policy",
  "apiVersion": "v1",
  "extenders": [
    {
      "urlPrefix":"<extender-endpoint>",
      "bindVerb": "bind",
      "managedResources": [
        {
          "name": "example.com/foo",
          "ignoredByScheduler": true
        }
      ]
    }
  ]
}
```

### 使用扩展资源

用户可以像CPU和内存一样在Pod的spec中使用扩展资源。调度器会进行资源计算，保证不会过量分配。

apiserver限制扩展资源的数量表达只能使用整数。比如*有效的*数量表达`3`、`3000m`和`3Ki`。比如*无效的*数量表达`0.5`、`1500m`。

>**注意**：扩展资源取代了非透明整数资源（Opaque Integer Resources）。用户可以使用除了`kubernetes.io`以外的任何域名前缀。

要想在Pod中使用扩展资源，将资源名作为一个key添加到`spec.containers[].resources.limits`中。

>**注意**：扩展资源不支持超量使用，所以如果同时在容器中设置了request和limit，它俩的值必须是一样的。

只有所有资源请求都满足的情况下才会对Pod进行调度，包括CPU、内存，以及其他任何扩展资源。只要资源请求无法满足，Pod就一直是`PENDING`状态。

**栗子**：

下面的Pod请求了2个CPU和1个“example.com/foo”（作为扩展资源）。

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: myimage
    resources:
      requests:
        cpu: 2
        example.com/foo: 1
      limits:
        example.com/foo: 1
```

## 排错
